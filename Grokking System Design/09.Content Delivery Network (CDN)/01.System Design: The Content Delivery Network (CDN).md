<h1>System Design: The Content Delivery Network (CDN)</h1>

<h2>Problem statement</h2>
Let’s start with a question: If millions of users worldwide use our data-intensive applications, 
and our service is deployed in a single data center to serve the users’ requests, what possible problems can arise?

The following problems can arise:
* High latency: The user-perceived latency will be high due to the physical distance from the serving data center. 
User-perceived latency has many components, such as transmission delays (a function of available bandwidth), 
propagation delays (a function of distance), queuing delays (a function of network congestion), 
and nodal processing delays. Therefore, data transmission over a large distance results in higher latency. 
Real-time applications require a latency below 200 milliseconds (ms) in general. For the Voice over Internet Protocol (VoIP), 
latency should not be more than 150 ms, whereas video streaming applications cannot tolerate a latency above a few seconds.

Note: According to one of the readings taken on December 21, 2021, the average latency from US East (N. Virginia) 
to US West (N. California) was 62.9 ms. Across continents—for example, from the US East (N. Virginia) 
to Africa (Cape Town)—was 225.63 ms. This is two-way latency, known as round-trip latency.

* Data-intensive applications: Data-intensive applications require transferring large traffic. Over a longer distance, 
this could be a problem due to the network path stretching through different kinds of ISPs. 
Because of some smaller Path message transmission unit (MTU) links, the throughput of applications on the network might be reduced. 
Similarly, different portions of the network path might have different congestion characteristics. 
The problem multiplies as the number of users grows because the origin servers will have to provide the data individually to each user. 
That is, the primary data center will need to send out a lot of redundant data when multiple clients ask for it. 
However, applications that use streaming services are both data-intensive and dynamic in nature.
* Scarcity of data center resources: Important data center resources like computational capacity and bandwidth become a limitation 
when the number of users of a service increases significantly. Services engaging millions of users simultaneously need scaling. 
Even if scaling is achieved in a single data center, it can still suffer from becoming a single point of failure 
when the data center goes offline due to natural calamity or connectivity issues with the Internet.
