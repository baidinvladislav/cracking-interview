Необходимо в операционной системе компьютера найти дубликаты файлов, 
дубликаты имеют собственное имя отличное от изначального названия файла.

Вернуть кортежи вида: [(duplicate_file_name.file_format), (original_file_name.file_format)]
Пример: [('/tmp/parker_is_dumb.mpg', '/home/parker/secret_puppy_dance.mpg'), ('/home/trololol.mov', '/etc/apache2/httpd.conf')]

Каждый файл был продублирован только однажды.

Нет идеи как приступить к этой задаче? Попытайся начать с чего то что-то, что пройдет по файловой системе компьютера 
и распечатает все названия файлов. Даже если не можешь реализовать рабочий код, который бы полностью решал задачу,
твой интервьюер все равно хочет узнать как размышляешь о проблеме.

Брутфорс решение здесь будет выглядить как: проитерировать все файлы в системе и для каждого файла проходить еще раз по списку файлов,
сравнивая файлы между собой, если они идентичны, значит дубликат найден. Это значит O(n^2) сравнений, кажется, можно лучше.

Как можно найти дубликаты за один проход по списку?

Вместо того, чтобы для одного файла искать дубликаты, мы можем отслеживать все файлы, которые мы выдели.
Какая структура данных хорошо подходит для этого?

Воспользуемся словарем. Когда мы увидем новый файл, мы сначала проверим есть ли файл в нашем словаре, если нет,
мы добавим его туда, если он там, то значит мы нашли дубликат.

Как определить, какой из файлов дубликат, а какой файл оригинал? Большинство операционных систем хранят время последнего изменения файла,
как метаданные. Тот файл, который был отредактирован последним, вероятно будет являться дубликатом.

Одно исключение: многим процессам нравится регулярно сохранять свое состояние в файл на диске, 
так что, если ваш компьютер внезапно выйдет из строя, процессы смогут продолжить более или менее с того места, 
на котором они остановились (именно так Word может сказать «похоже на у вас были несохраненные изменения в прошлый раз, хотите восстановить их?»).

Если вы продублировали некоторые из этих файлов, последний из них может не быть дубликатом. 
Но, рискуя сломать нашу систему (очевидно, мы сначала сделаем резервную копию), мы будем работать с эвристикой «последняя отредактированная копия файла, 
вероятно, дубликат».

Наша функция пройдет через файловую систему, сохраняя файлы в словарь, идентифицируя последний отредактированный файл как дубликат.

```python
# словарь для хранения файлов, которые мы уже видели стек
# стек для хранения каталогов и файлов по мере их прохождения
# список для хранения наших выходных кортежей

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

    return duplicates

```

Мы собираемся сделать нашу функцию итеративной, а не рекурсивной, чтобы избежать переполнения стека.

```python
  import os

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

        # If it's a directory,
        # put the contents in our stack
        if os.path.isdir(current_path):
            for path in os.listdir(current_path):
                full_path = os.path.join(current_path, path)
                stack.append(full_path)

        # If it's a file
        else:
            # Get its contents
            with open(current_path) as file:
                file_contents = file.read()

            # Get its last edited time
            current_last_edited_time = os.path.getmtime(current_path)

            # If we've seen it before
            if file_contents in files_seen_already:
                existing_last_edited_time, existing_path = files_seen_already[file_contents]
                if current_last_edited_time > existing_last_edited_time:
                    # Current file is the dupe!
                    duplicates.append((current_path, existing_path))
                else:
                    # Old file is the dupe!
                    # So delete it
                    duplicates.append((existing_path, current_path))
                    # But also update files_seen_already to have
                    # the new file's info
                    files_seen_already[file_contents] = (current_last_edited_time, current_path)

            # If it's a new file, throw it in files_seen_already
            # and record the path and the last edited time,
            # so we can delete it later if it's a dupe
            else:
                files_seen_already[file_contents] = (current_last_edited_time, current_path)

    return duplicates

```

Окей, это будет работать! Какая временная и пространственная сложность?

Мы помещаем полное содержимое каждого файла в наш словарь! Это стоит O(b) времени и места, где b — это общий объем пространства, 
занимаемый всеми файлами в файловой системе.

Эта стоимость места довольно объемна — нам нужно хранить дубликат всей нашей файловой системы (например, 
несколько гигабайт только видео с кошками) в рабочей памяти!

Можем ли мы уменьшить стоимость места? Что, если мы согласны с некоторой потерей точности (например, 
мы делаем более «нечеткое» сопоставление, чтобы увидеть, совпадают ли два файла)?

Что, если вместо того, чтобы создавать ключи для словаря всем содержимым файла, мы сначала хешируем это содержимое? 
Таким образом, мы бы сохранили «отпечаток» файла постоянного размера в нашем словаре, а не сам файл целиком. 
Это дало бы нам O(1) места на файл (всего O(n) места, где n — количество файлов)!

Это огромное улучшение. Но мы можем пойти дальше! Пока мы делаем сопоставление файлов «нечетким», 
можем ли мы использовать аналогичную идею, чтобы сэкономить время? Обратите внимание, 
что стоимость нашего времени по-прежнему соответствует общему размеру файлов на диске, а стоимость пространства — порядку количества файлов.

Для каждого файла мы должны просмотреть каждый бит, который занимает файл, чтобы хешировать его и снять «отпечаток пальца». 
Вот почему наши затраты времени высоки. Можем ли мы вместо этого отпечатать файл за постоянное время? 
Что, если вместо хеширования всего содержимого каждого файла мы хэшировали три «выборки» фиксированного размера из каждого файла, 
состоящего из первых X байтов, средних X байтов и последних X байтов? Это позволит нам отпечатывать файл за постоянное время!

Насколько большими мы должны сделать наши «выборки»?

Когда ваш диск выполняет чтение, он захватывает содержимое кусками постоянного размера, называемыми «блоками».

Насколько велики блоки? Это зависит от файловой системы. Мой Macintosh использует файловую систему под названием HFS+, 
которая имеет размер блока по умолчанию 4 КБ (4000 байт) на блок.

Таким образом, мы могли бы использовать всего по 100 байт от начала, середины и конца наших файлов, но каждый раз, 
когда мы захватывали эти байты, наш диск на самом деле захватывал бы 4000 байт, а не только 100 байт. 
Мы бы просто выбросили остальные. С тем же успехом мы могли бы использовать их все, так как большее количество контента файла 
помогает нам гарантировать уникальность отпечатков пальцев. Таким образом, наши образцы должны быть размером с размер блока нашей файловой системы.

Мы итеративно проходим через всю нашу файловую систему. По мере продвижения мы берем «отпечаток пальца» каждого файла за постоянное время, 
хешируя первые X, X средних и X последних байтов, в зависимости от нашей размера захватываемого блока нашей ОС. 
По мере продвижения мы сохраняем отпечаток каждого файла в словаре.

Если отпечаток данного файла уже есть в нашем словаре, мы предполагаем, что у нас есть дубликат. В этом случае мы предполагаем, 
что файл, отредактированный последним, является дубликатом.

```python
import os
import hashlib

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

        # If it's a directory,
        # put the contents in our stack
        if os.path.isdir(current_path):
            for path in os.listdir(current_path):
                full_path = os.path.join(current_path, path)
                stack.append(full_path)

        # If it's a file
        else:
            # Get its hash
            file_hash = sample_hash_file(current_path)

            # Get its last edited time
            current_last_edited_time = os.path.getmtime(current_path)

            # If we've seen it before
            if file_hash in files_seen_already:
                existing_last_edited_time, existing_path = files_seen_already[file_hash]
                if current_last_edited_time > existing_last_edited_time:
                    # Current file is the dupe!
                    duplicates.append((current_path, existing_path))
                else:
                    # Old file is the dupe!
                    duplicates.append((existing_path, current_path))
                    # But also update files_seen_already to have
                    # the new file's info
                    files_seen_already[file_hash] = (current_last_edited_time, current_path)

            # If it's a new file, throw it in files_seen_already
            # and record its path and last edited time,
            # so we can tell later if it's a dupe
            else:
                files_seen_already[file_hash] = (current_last_edited_time, current_path)

    return duplicates


def sample_hash_file(path):
    num_bytes_to_read_per_sample = 4000
    total_bytes = os.path.getsize(path)
    hasher = hashlib.sha512()

    with open(path, 'rb') as file:
        # If the file is too short to take 3 samples, hash the entire file
        if total_bytes < num_bytes_to_read_per_sample * 3:
            hasher.update(file.read())
        else:
            num_bytes_between_samples = (
                (total_bytes - num_bytes_to_read_per_sample * 3) / 2
            )

            # Read first, middle, and last bytes
            for offset_multiplier in range(3):
                start_of_sample = (
                    offset_multiplier
                    * (num_bytes_to_read_per_sample + num_bytes_between_samples)
                )
                file.seek(start_of_sample)
                sample = file.read(num_bytes_to_read_per_sample)
                hasher.update(sample)

    return hasher.hexdigest()

```
