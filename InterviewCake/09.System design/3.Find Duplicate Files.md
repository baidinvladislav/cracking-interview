Необходимо в операционной системе компьютера найти дубликаты файлов, 
дубликаты имеют собственное имя отличное от изначального названия файла.

Вернуть кортежи вида: [(duplicate_file_name.file_format), (original_file_name.file_format)]
Пример: [('/tmp/parker_is_dumb.mpg', '/home/parker/secret_puppy_dance.mpg'), ('/home/trololol.mov', '/etc/apache2/httpd.conf')]

Каждый файл был продублирован только однажды.

Нет идеи как приступить к этой задаче? Попытайся начать с чего то что-то, что пройдет по файловой системе компьютера 
и распечатает все названия файлов. Даже если не можешь реализовать рабочий код, который бы полностью решал задачу,
твой интервьюер все равно хочет узнать как размышляешь о проблеме.

Брутфорс решение здесь будет выглядить как: проитерировать все файлы в системе и для каждого файла проходить еще раз по списку файлов,
сравнивая файлы между собой, если они идентичны, значит дубликат найден. Это значит O(n^2) сравнений, кажется, можно лучше.

Как можно найти дубликаты за один проход по списку?

Вместо того, чтобы для одного файла искать дубликаты, мы можем отслеживать все файлы, которые мы выдели.
Какая структура данных хорошо подходит для этого?

Воспользуемся словарем. Когда мы увидем новый файл, мы сначала проверим есть ли файл в нашем словаре, если нет,
мы добавим его туда, если он там, то значит мы нашли дубликат.

Как определить, какой из файлов дубликат, а какой файл оригинал? Большинство операционных систем хранят время последнего изменения файла,
как метаданные. Тот файл, который был отредактирован последним, вероятно будет являться дубликатом.

Одно исключение: многим процессам нравится регулярно сохранять свое состояние в файл на диске, 
так что, если ваш компьютер внезапно выйдет из строя, процессы смогут продолжить более или менее с того места, 
на котором они остановились (именно так Word может сказать «похоже на у вас были несохраненные изменения в прошлый раз, хотите восстановить их?»).

Если вы продублировали некоторые из этих файлов, последний из них может не быть дубликатом. 
Но, рискуя сломать нашу систему (очевидно, мы сначала сделаем резервную копию), мы будем работать с эвристикой «последняя отредактированная копия файла, 
вероятно, дубликат».

Наша функция пройдет через файловую систему, сохраняя файлы в словарь, идентифицируя последний отредактированный файл как дубликат.

```python
# словарь для хранения файлов, которые мы уже видели стек
# стек для хранения каталогов и файлов по мере их прохождения
# список для хранения наших выходных кортежей

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

    return duplicates

```

Мы собираемся сделать нашу функцию итеративной, а не рекурсивной, чтобы избежать переполнения стека.

```python
  import os

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

        # If it's a directory,
        # put the contents in our stack
        if os.path.isdir(current_path):
            for path in os.listdir(current_path):
                full_path = os.path.join(current_path, path)
                stack.append(full_path)

        # If it's a file
        else:
            # Get its contents
            with open(current_path) as file:
                file_contents = file.read()

            # Get its last edited time
            current_last_edited_time = os.path.getmtime(current_path)

            # If we've seen it before
            if file_contents in files_seen_already:
                existing_last_edited_time, existing_path = files_seen_already[file_contents]
                if current_last_edited_time > existing_last_edited_time:
                    # Current file is the dupe!
                    duplicates.append((current_path, existing_path))
                else:
                    # Old file is the dupe!
                    # So delete it
                    duplicates.append((existing_path, current_path))
                    # But also update files_seen_already to have
                    # the new file's info
                    files_seen_already[file_contents] = (current_last_edited_time, current_path)

            # If it's a new file, throw it in files_seen_already
            # and record the path and the last edited time,
            # so we can delete it later if it's a dupe
            else:
                files_seen_already[file_contents] = (current_last_edited_time, current_path)

    return duplicates

```

Окей, это будет работать! Какая временная и пространственная сложность?

Мы помещаем полное содержимое каждого файла в наш словарь! Это стоит O(b) времени и места, где b — это общий объем пространства, 
занимаемый всеми файлами в файловой системе.

Эта стоимость места довольно объемна — нам нужно хранить дубликат всей нашей файловой системы (например, 
несколько гигабайт только видео с кошками) в рабочей памяти!

Можем ли мы уменьшить стоимость места? Что, если мы согласны с некоторой потерей точности (например, 
мы делаем более «нечеткое» сопоставление, чтобы увидеть, совпадают ли два файла)?

Что, если вместо того, чтобы создавать ключи для словаря всем содержимым файла, мы сначала хешируем это содержимое? 
Таким образом, мы бы сохранили «отпечаток» файла постоянного размера в нашем словаре, а не сам файл целиком. 
Это дало бы нам O(1) места на файл (всего O(n) места, где n — количество файлов)!

Это огромное улучшение. Но мы можем пойти дальше! Пока мы делаем сопоставление файлов «нечетким», 
можем ли мы использовать аналогичную идею, чтобы сэкономить время? Обратите внимание, 
что стоимость нашего времени по-прежнему соответствует общему размеру файлов на диске, а стоимость пространства — порядку количества файлов.

Для каждого файла мы должны просмотреть каждый бит, который занимает файл, чтобы хешировать его и снять «отпечаток пальца». 
Вот почему наши затраты времени высоки. Можем ли мы вместо этого отпечатать файл за постоянное время? 
Что, если вместо хеширования всего содержимого каждого файла мы хэшировали три «выборки» фиксированного размера из каждого файла, 
состоящего из первых X байтов, средних X байтов и последних X байтов? Это позволит нам отпечатывать файл за постоянное время!

Насколько большими мы должны сделать наши «выборки»?

Когда ваш диск выполняет чтение, он захватывает содержимое кусками постоянного размера, называемыми «блоками».

Насколько велики блоки? Это зависит от файловой системы. Мой Macintosh использует файловую систему под названием HFS+, 
которая имеет размер блока по умолчанию 4 КБ (4000 байт) на блок.

Таким образом, мы могли бы использовать всего по 100 байт от начала, середины и конца наших файлов, но каждый раз, 
когда мы захватывали эти байты, наш диск на самом деле захватывал бы 4000 байт, а не только 100 байт. 
Мы бы просто выбросили остальные. С тем же успехом мы могли бы использовать их все, так как большее количество контента файла 
помогает нам гарантировать уникальность отпечатков пальцев. Таким образом, наши образцы должны быть размером с размер блока нашей файловой системы.

Мы итеративно проходим через всю нашу файловую систему. По мере продвижения мы берем «отпечаток пальца» каждого файла за постоянное время, 
хешируя первые X, X средних и X последних байтов, в зависимости от нашей размера захватываемого блока нашей ОС. 
По мере продвижения мы сохраняем отпечаток каждого файла в словаре.

Если отпечаток данного файла уже есть в нашем словаре, мы предполагаем, что у нас есть дубликат. В этом случае мы предполагаем, 
что файл, отредактированный последним, является дубликатом.

```python
import os
import hashlib

def find_duplicate_files(starting_directory):
    files_seen_already = {}
    stack = [starting_directory]

    # We'll track tuples of (duplicate_file, original_file)
    duplicates = []

    while len(stack) > 0:
        current_path = stack.pop()

        # If it's a directory,
        # put the contents in our stack
        if os.path.isdir(current_path):
            for path in os.listdir(current_path):
                full_path = os.path.join(current_path, path)
                stack.append(full_path)

        # If it's a file
        else:
            # Get its hash
            file_hash = sample_hash_file(current_path)

            # Get its last edited time
            current_last_edited_time = os.path.getmtime(current_path)

            # If we've seen it before
            if file_hash in files_seen_already:
                existing_last_edited_time, existing_path = files_seen_already[file_hash]
                if current_last_edited_time > existing_last_edited_time:
                    # Current file is the dupe!
                    duplicates.append((current_path, existing_path))
                else:
                    # Old file is the dupe!
                    duplicates.append((existing_path, current_path))
                    # But also update files_seen_already to have
                    # the new file's info
                    files_seen_already[file_hash] = (current_last_edited_time, current_path)

            # If it's a new file, throw it in files_seen_already
            # and record its path and last edited time,
            # so we can tell later if it's a dupe
            else:
                files_seen_already[file_hash] = (current_last_edited_time, current_path)

    return duplicates


def sample_hash_file(path):
    num_bytes_to_read_per_sample = 4000
    total_bytes = os.path.getsize(path)
    hasher = hashlib.sha512()

    with open(path, 'rb') as file:
        # If the file is too short to take 3 samples, hash the entire file
        if total_bytes < num_bytes_to_read_per_sample * 3:
            hasher.update(file.read())
        else:
            num_bytes_between_samples = (
                (total_bytes - num_bytes_to_read_per_sample * 3) / 2
            )

            # Read first, middle, and last bytes
            for offset_multiplier in range(3):
                start_of_sample = (
                    offset_multiplier
                    * (num_bytes_to_read_per_sample + num_bytes_between_samples)
                )
                file.seek(start_of_sample)
                sample = file.read(num_bytes_to_read_per_sample)
                hasher.update(sample)

    return hasher.hexdigest()

```

Здесь мы сделали несколько предположений:
* Два разных файла не будут иметь одинаковых отпечатков пальцев. 
Не исключено, что два файла с разным содержимым будут иметь одинаковые начальный, средний и конечный байты, 
поэтому у них будут одинаковые отпечатки пальцев. Или они могут даже иметь разные байты выборки, 
но по-прежнему хэшировать до одного и того же значения (это называется «коллизией хэшей»). Чтобы смягчить это, 
мы могли бы выполнять проверку в последнюю минуту всякий раз, когда находим два «совпадающих» файла, 
где мы фактически сканируем полное содержимое файла, чтобы увидеть, совпадают ли они.
* Последний отредактированный файл является дубликатом. Это кажется разумным, но может быть неверным — например, 
могут быть файлы, которые были отредактированы демонами (программами, работающими в фоновом режиме)
* Два файла с одинаковым содержимым — это один и тот же файл. Это кажется тривиальной правдой, 
но может вызвать некоторые проблемы. Например, у нас могут быть пустые файлы в нескольких местах нашей файловой системы, 
которые не являются дубликатами друг друга.


Учитывая эти потенциальные проблемы, мы определенно хотим, чтобы человек подтвердил, прежде чем мы удалим какие-либо файлы. 
Тем не менее, это намного лучше, чем прочесывать всю нашу файловую систему вручную!

Некоторые идеи для дальнейших улучшений:
1. Если файл не редактировался в последний раз примерно в то время, когда ваш друг завладел вашим компьютером, 
вы знаете, что он, вероятно, не был создан вашим другом. Точно так же, если к файлу не обращались 
(иногда ваша файловая система также хранит время последнего доступа к файлу) примерно в это время, вы знаете, 
что он не был скопирован вашим другом. Вы можете использовать эти факты, чтобы пропустить некоторые файлы.
2. Сделайте размер файла отпечатком пальца — он должен быть доступен в виде метаданных в файле 
(чтобы вам не нужно было просматривать весь файл, чтобы узнать, насколько он длинный). 
Вы получите много ложных срабатываний, но это нормально, если вы относитесь к этому как к шагу «предварительной обработки». 
Возможно, затем вы берете отпечатки пальцев на основе хэшей только для файлов с соответствующими размерами. 
Затем вы полностью сравниваете содержимое файлов, если они имеют одинаковый хэш.
3. Некоторые файловые системы также отслеживают время создания файла. Если ваша файловая система поддерживает это, 
вы можете использовать это как потенциально более сильную эвристику для определения того, 
какая из двух копий файла является дубликатом.
4. Когда вы сравниваете полное содержимое файла, чтобы убедиться, что два файла одинаковы, 
нет необходимости считывать все файлы в память. Откройте оба файла и прочитайте их по одному блоку за раз. 
Вы можете закоротить, как только найдете два несовпадающих блока, и вам всегда нужно хранить в памяти только пару блоков.

<b>Сложность</b>
Каждый хэш занимает O(1) времени и места, поэтому наши общие затраты времени и места составляют O(n),
где n — количество файлов в файловой системе.

Если мы добавим проверку в последнюю минуту, чтобы увидеть, действительно ли два файла с одинаковыми отпечатками пальцев 
являются одними и теми же файлами (что, вероятно, и следует сделать), то в худшем случае все файлы будут одинаковыми, 
и нам придется прочитать их полное содержимое, чтобы подтвердить это дает нам время выполнения порядка общего размера наших файлов на диске.

<b>Бонус</b>
Если бы мы хотели подготовить этот код для производственной системы, мы могли бы сделать его немного более модульным. 
Попробуйте отделить код обхода файла от кода обнаружения дубликатов. Попробуйте реализовать обход файла с помощью генератора!

Как насчет параллелизма? Можем ли мы работать быстрее, разделив эту процедуру на несколько потоков? 
Кроме того, что, если фоновый процесс редактирует файл во время работы нашего скрипта? Это вызовет проблемы?

Как насчет файлов ссылок (файлов, которые указывают на другие файлы или папки)? Одна загвоздка здесь в том, 
что файл ссылки может указывать на дерево файлов. Как сделать так, чтобы обход файла не шел по кругу?

<b>Инсайд</b>
Основная идея заключалась в том, чтобы сэкономить время и место, «сняв отпечатки пальцев» с каждого файла.

Этот вопрос — хороший пример «грязной» проблемы на собеседовании. Вместо одного оптимального решения есть большой 
узел оптимизаций и компромиссов. Например, наш подход, основанный на хешировании, обеспечивает более быстрое время выполнения, 
но может давать ложные срабатывания.

Для таких запутанных проблем, как эта, сосредоточьтесь на том, чтобы четко объяснить интервьюеру, 
каковы компромиссы для каждого решения, которое вы принимаете. Фактический выбор, который вы делаете, 
вероятно, не имеет большого значения, если вы демонстрируете сильную способность понимать и сравнивать свои варианты.
