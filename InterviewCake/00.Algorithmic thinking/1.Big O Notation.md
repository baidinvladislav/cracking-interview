## Big O Notation
<h4>Значение большого О</h4>

Большое О - это "язык", который используется для определения того как долго работает алгоритм.

При помощи натации большого О мы можем выразить как быстро растет сложность алгоритма по мере роста входных данных.

1. Сложно сказать сколько точно займет выполнение какого либо алгоритма, потому что 
это зависит от скорости процессора, от его загрузки и т.д. Поэтому вместо того, чтобы говорить о времени выполнения конкретно
мы используем измереяем сложность в большой О нотации, чтобы определить насколько быстро растет сложность алгоритма.
2. Если бы измеряли скорость выполнения алгоритма, то мы измеряли время в секундах, но так как мы измеряем как быстро растет сложность,
то нам необходима другая единица изменерения. В большой О нотации мы используем размер входных данных для этого, мы можем
сказать, что сложность алгоритма растет линейно с увеличением входных данных или можем сказать, что сложность алгоритма
растет квадратично по мере роста входных данных.
3. Наш алгоритм может иметь шаги, которые кажутся довольно "дешевыми" на маленьких входных данных, но 
те же самые шаги могут быть очень дороги по мере роста входных данных.

If this seems abstract so far, that's because it is. Let's look at some examples.


<h2>Some examples</h2>

```python
def print_first_item(items):
    print(items[0])

```
  

<b>Эта ф-ия имеет константную или О(1) сложность</b>. Входящий массив может содержать 1 элемент, а может и 1000 элементов,
но ф-ия всегда будет требовать один шаг.

```python
def print_all_items(items):
    for item in items:
        print(item)

```

<b>Эта ф-ия имеет линейную или O(n) сложность, где n это количество элементов с списке.</b> Если список сотоит из 10 элементов, 
мы вызовем ф-ию print 10 раз. Если в списке 1.000 элементов, мы вызовем ф-ию print 1.000 раз.

```python
def print_all_possible_ordered_pairs(items):
    for first_item in items:
        for second_item in items:
            print(first_item, second_item)

```

<b>В этом примере у нас вложенный цикл в цикл. Если в списке n элементов, наш внешний цикл сделает n итераций, а также наш внутренний цикл сделает n итерации,
что в итоге нам даст квадратичное или O(n^2) сложность алгоритма.</b>. 
Если в списке 10 жлементов, мы вызовем ф-ию print 100 раз. Если у нас в списке 1.000 элементов, мы вызовем print 1.000.000 раз.


<h2>N может быть самим инпутом, а может быть размером инпута</h2>

Обе эти функции имеют сложность O(n), даже несмотря на то что одна принимает число, а вторая список:

```python
def say_hi_n_times(n):
    for time in range(n):
        print("hi")


def print_all_items(items):
    for item in items:
        print(item)

```

Так иногда n это число переданное в функции, а иногда коллекция элементов.

<h2>Константами можно принебречь</h2>

Когда мы говорим об большой О мы можем отбрасывать константы:

```python
def print_all_items_twice(items):
    for item in items:
        print(item)
    
    # Once more, with feeling
    for item in items:
        print(item)

```

Эта функция имеет O(2n) сложность, мы можем просто сказать O(n).

```python
def print_first_item_then_first_half_then_say_hi_100_times(items):
    print(items[0])
    
    middle_index = len(items) // 2
    index = 0
    
    while index < middle_index:
        print(items[index])
        index += 1
    
    for time in range(100):
        print("hi")

```

В этом примере ф-ия работает за O(1 + n/2 + 100), но мы по прежнему получаем O(n) без констант.

Почему мы можем отбросить константы? Так как большая О представляет сложность, когда алгоритм работает с большими входящими данными, 
то увеличение сложности на константу раз (2 или 100 в примере выше) не настолько значительно, что может быть отброшено.

<h2>Drop the less significant terms</h2>

For example:
```python
def print_all_numbers_then_all_pair_sums(numbers):
  print("these are the numbers:")
  for number in numbers:
      print(number)

  print("and these are their sums:")
  for first_number in numbers:
      for second_number in numbers:
          print(first_number + second_number)

```


Here our runtime is O(n + n^2), which we just call O(n^2). Even if it was O(n^2 / 2 + 100n), it would still be O(n^2).

Similarly:
O(n^3 + 50n^2 + 1000) is O(n^3)
O(n + 30) * (n + 5) is O(n^2)

Again, we can get away with this because the less significant terms quickly become, well, less significant as nn gets big.


We're usually talking about the "worst case"

Often this "worst case" stipulation is implied. But sometimes you can impress your interviewer by saying it explicitly.

Sometimes the worst case runtime is significantly worse than the best case runtime:

```python
def contains(haystack, needle):

    # Does the haystack contain the needle?
    for item in haystack:
        if item == needle:
            return True
    
    return False

```

Here we might have 100 items in our haystack, but the first item might be the needle, in which case we would return in just 1 iteration of our loop.

In general we'd say this is O(n) runtime and the "worst case" part would be implied. But to be more specific we could say this is worst case O(n) and best case O(1) runtime. 
For some algorithms we can also make rigorous statements about the "average case" runtime.

<h2>Space complexity: the final frontier</h2>


Sometimes we want to optimize for using less memory instead of (or in addition to) using less time. Talking about memory cost (or "space complexity") is very similar to talking about time cost. 
We simply look at the total size (relative to the size of the input) of any new variables we're allocating.

This function takes O(1) space (we use a fixed number of variables):

```python
def say_hi_n_times(n):
    for time in range(n):
        print("hi")

```

This function takes O(n) space (the size of hi_list scales with the size of the input):

```python
def list_of_hi_n_times(n):
    hi_list = []
    for time in range(n):
        hi_list.append("hi")
    return hi_list

```

<b>Usually when we talk about space complexity, we're talking about additional space</b>, so we don't include space taken up by the inputs. 
For example, this function takes constant space even though the input has n items:

```python
def get_largest_item(items):
    largest = float('-inf')
    for item in items:
        if item > largest:
            largest = item
    return largest

```

<b>Sometimes there's a tradeoff between saving time and saving space</b>, so you have to decide which one you're optimizing for.


<h2>Big O analysis is awesome except when it's not</h2>

You should make a habit of thinking about the time and space complexity of algorithms as you design them. Before long this'll become second nature, allowing you to see optimizations and potential performance issues right away.
Asymptotic analysis is a powerful tool, but wield it wisely.
Big O ignores constants, but <b>sometimes the constants matter</b>. If we have a script that takes 5 hours to run, an optimization that divides the runtime by 5 might not affect big O, but it still saves you 4 hours of waiting.

<b>Beware of premature optimization.</b>Sometimes optimizing time or space negatively impacts readability or coding time. For a young startup it might be more important to write code that's easy to ship quickly or easy to understand later, even if this means it's less time and space efficient than it could be.
But that doesn't mean startups don't care about big O analysis. A great engineer (startup or otherwise) knows how to strike the right balance between runtime, space, implementation time, maintainability, and readability.
<b>You should develop the skill to see time and space optimizations, as well as the wisdom to judge if those optimizations are worthwhile.</b>


